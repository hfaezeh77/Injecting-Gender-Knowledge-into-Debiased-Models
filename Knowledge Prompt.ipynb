{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ArgvoTQ9sUh",
        "outputId": "1728adf8-11c1-4178-fc7f-b43090ee73f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.15.1 tokenizers-0.13.3 transformers-4.29.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import random"
      ],
      "metadata": {
        "id": "ZWCejvm69fgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXMDoj-X8xGH",
        "outputId": "bd29ef9f-7044-4403-cef2-932374749d8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('/content/drive/MyDrive/NLP project/Gender_specific_data')"
      ],
      "metadata": {
        "id": "ACqdyH2v9GeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Dataset"
      ],
      "metadata": {
        "id": "Q9nfiEnWf7s-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the dataset class\n",
        "class TripleDataset(Dataset):\n",
        "  def __init__(self, triples):\n",
        "    self.triples = triples\n",
        "    self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.triples)\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "    triple = self.triples.iloc[idx]\n",
        "    subject,relation,object = eval(triple)\n",
        "\n",
        "    relation = relation.strip(',')\n",
        "    subject = subject.strip(',')\n",
        "    object = object.strip(',')\n",
        "\n",
        "    input_text = f\"{subject} {relation} {object}\"\n",
        "    masked_input_text = f\"[MASK] {relation} {object}\"\n",
        "    # inputs = self.tokenizer(input_text, return_tensor='pt',paddng=True, truncation=True)\n",
        "    encoded = self.tokenizer.encode_plus(input_text, add_special_tokens=True, padding='max_length', max_length=128,\n",
        "                                         truncation=True, return_tensors='pt')\n",
        "    input_ids=encoded['input_ids'].squeeze(0)\n",
        "    # attention_mask=encoded['attention_mask'].squeeze(0)\n",
        "    # token_type_ids=encoded['token_type_ids'].squeeze(0)\n",
        "\n",
        "    encoded2 = self.tokenizer.encode_plus(masked_input_text, add_special_tokens=True, padding='max_length', max_length=128,\n",
        "                                         truncation=True, return_tensors='pt')\n",
        "    masked_input_ids=encoded2['input_ids'].squeeze(0)\n",
        "    attention_mask=encoded2['attention_mask'].squeeze(0)\n",
        "    token_type_ids=encoded2['token_type_ids'].squeeze(0)\n",
        "\n",
        "    return {'labels': input_ids,\n",
        "            'input_ids': masked_input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'token_type_ids': token_type_ids,\n",
        "            'subject': subject,\n",
        "            'object': object\n",
        "            }"
      ],
      "metadata": {
        "id": "XyZv2tm59izJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def collate_fn(batch):\n",
        "#     tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "#     input_ids = [item['input_ids'] for item in batch]\n",
        "#     masked_input_ids = [item['masked_input_ids'] for item in batch]\n",
        "#     attention_mask = [item['attention_mask'] for item in batch]\n",
        "#     token_type_ids = [item['token_type_ids'] for item in batch]\n",
        "#     subjects = [item['subject'] for item in batch]\n",
        "#     objects = [item['obj'] for item in batch]\n",
        "\n",
        "#     input_ids = pad_sequence(input_ids, batch_first=True)\n",
        "#     attention_mask = pad_sequence(attention_mask, batch_first=True)\n",
        "#     token_type_ids = pad_sequence(token_type_ids, batch_first=True)\n",
        "\n",
        "#     mask_token_id = tokenizer.mask_token_id\n",
        "#     labels = input_ids.clone()\n",
        "\n",
        "#     for i, subject in enumerate(subjects):\n",
        "#         subject_start = (input_ids[i] == tokenizer.encode('[CLS]', add_special_tokens=False)[0]).nonzero(as_tuple=False)[0]\n",
        "#         subject_end = (input_ids[i] == tokenizer.encode('[SEP]', add_special_tokens=False)[0]).nonzero(as_tuple=False)[-1] + 1\n",
        "#         subject_tokens = tokenizer.encode(subject, add_special_tokens=False)\n",
        "#         input_ids[i, subject_start:subject_end] = mask_token_id\n",
        "#         labels[i, subject_start:subject_end] = input_ids[i, subject_start:subject_end]\n",
        "\n",
        "#     return {\n",
        "#         'input_ids': input_ids,\n",
        "#         'attention_mask': attention_mask,\n",
        "#         'token_type_ids': token_type_ids,\n",
        "#         'labels': labels,\n",
        "#         'subjects': subjects,\n",
        "#         'objects': objects\n",
        "#     }\n"
      ],
      "metadata": {
        "id": "ihQBFvwm-IWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-1VqtWrNJbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make the dataloader\n",
        "triples = data['triple']\n",
        "dataset = TripleDataset(triples)\n",
        "dataloader = DataLoader(dataset, batch_size=32)"
      ],
      "metadata": {
        "id": "ZMIE47U5gDiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.decode(dataset[100]['input_ids'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "wPO_al8eMdl_",
        "outputId": "87b490cd-ff75-41e2-f762-586fedf9f468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'[CLS] [MASK] father wossen seged [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "dataset[30]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6W0EGPd9v1rO",
        "outputId": "440e990e-872c-4a37-e993-6b3237181611"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': tensor([  101, 19542,  7274,  2271,  2388, 29347,  9354,   102,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'input_ids': tensor([  101,   103,  2388, 29347,  9354,   102,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "             0,     0,     0,     0,     0,     0,     0,     0]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0]),\n",
              " 'subject': 'Dionysus',\n",
              " 'object': 'Aegle'}"
            ]
          },
          "metadata": {},
          "execution_count": 226
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Soft Prompt Configuration\n",
        "soft_prompt = torch.randn(1, 128, 768)  # Replace with your desired soft prompt"
      ],
      "metadata": {
        "id": "sSLe1XvtuKgb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the model"
      ],
      "metadata": {
        "id": "aSfUANxff4Zc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Knowledge Prompt (KP) Model\n",
        "class KPModel(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(KPModel, self).__init__()\n",
        "        self.hidden_layer = torch.nn.Linear(input_dim, hidden_dim)\n",
        "        self.activation = torch.nn.ReLU()\n",
        "        self.output_layer = torch.nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.hidden_layer(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        ""
      ],
      "metadata": {
        "id": "cNLoIQTGuQnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Language Model\n",
        "lm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "for param in lm_model.parameters():\n",
        "    param.requires_grad = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTMum6EAuU7U",
        "outputId": "6cf9b6cc-b70b-40a4-f218-349e74305527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print('device:',device)\n",
        "kp_model = KPModel(input_dim=32, hidden_dim=512, output_dim=384)\n",
        "kp_model.to(device)\n",
        "\n",
        "optimizer = torch.optim.AdamW(kp_model.parameters(), lr=1e-5)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "num_epochs=5\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBSbaeUcuk6z",
        "outputId": "0e147c53-1b64-4672-e208-ccea1804167e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Soft Prompt Configuration\n",
        "soft_prompt = torch.randn(1, 1, 768)  # Soft prompt with dimensions (1, 1, 768)"
      ],
      "metadata": {
        "id": "IPGyDgAR466b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the KP"
      ],
      "metadata": {
        "id": "VDyZJMznQ1Dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for batch in dataloader:\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    token_type_ids = batch['token_type_ids'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    subjects = batch['subject']\n",
        "    objects = batch['object']\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for subject in subjects:\n",
        "        subject_tokens = tokenizer.encode(subject, add_special_tokens=False)  # Encode the subject\n",
        "\n",
        "        # Convert subject_tokens to a tensor\n",
        "        subject_tensor = torch.tensor(subject_tokens, dtype=torch.float32).unsqueeze(0)  # Unsqueeze to add a batch dimension\n",
        "\n",
        "        # Repeat subject_tensor to match the batch size of input_ids\n",
        "        subject_tensor = subject_tensor.repeat(input_ids.size(0), 8)\n",
        "\n",
        "        # Pass the subject to the KP model\n",
        "        kp_outputs = kp_model(subject_tensor)\n",
        "\n",
        "        # Concatenate input_ids with kp_outputs_repeated\n",
        "        print(input_ids.size())\n",
        "        print(kp_outputs.size())\n",
        "        input_with_kp = torch.cat([input_ids, kp_outputs.to(input_ids.dtype)], dim=-1)\n",
        "\n",
        "        lm_model.eval()\n",
        "\n",
        "        # Pass the input with KP to the language model\n",
        "        print(input_with_kp.size())\n",
        "        attention_mask = attention_mask.float()\n",
        "        outputs = lm_model(input_ids=input_with_kp, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "\n",
        "        lm_model.train()\n",
        "\n",
        "        # Calculate loss based on the predicted subject/object and the actual subject/object\n",
        "        predicted_subjects = outputs.logits[:, :-1, :][attention_mask[:, :-1] == 1]\n",
        "        predicted_objects = outputs.logits[:, 1:, :][attention_mask[:, 1:] == 1]\n",
        "\n",
        "        subject_loss = criterion(predicted_subjects, subjects)\n",
        "        object_loss = criterion(predicted_objects, objects)\n",
        "\n",
        "        kp_loss = subject_loss + object_loss\n",
        "\n",
        "        total_kp_loss += kp_loss.item() * input_ids.size(0)\n",
        "        total_examples += input_ids.size(0)\n",
        "\n",
        "        kp_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "avg_kp_loss = total_kp_loss / total_examples\n",
        "print(f\"Epoch: {epoch + 1}/{num_epochs}, KP Loss: {avg_kp_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "ke-GITJd5SBg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4r2iFwHXJFfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}